{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce76b35-873d-44ff-848e-2605ed94de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac45f1d-d7e0-4211-b0c9-6a3ea7a437f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ab0709df9649359759b292ded1fc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.3\"\n",
    "#MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#MODEL_NAME = \"microsoft/phi-2\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer with reduced memory usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27beb27e-dd87-41b5-8dcf-9d5cd37448c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: what is the meaning of life?\n",
      "\n",
      "I’m not sure if I’ve ever asked myself this question. I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been\n"
     ]
    }
   ],
   "source": [
    "# Test a simple prompt\n",
    "prompt = \"what is the meaning of life?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea9069-ee33-4826-869a-851625734281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f498da9-b943-4906-8bdc-eddf9fc91a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: what is quantization on large language models?\n",
      "\n",
      "Quantization is a technique used to reduce the computational requirements of a model by reducing the precision of its parameters. In the context of large language models, quantization can be used to reduce the number of bits used to represent the model’s parameters, which can lead to significant reductions in the amount of memory and computational resources required to run the model.\n",
      "\n",
      "There are several different types of quantization techniques that can be used to reduce the precision of a model’s parameters. One common approach is to use a technique called “binary quantization,” which involves representing the parameters of the model using only two possible values (0 and 1). This can be done by using a technique called “binary encoding,” which involves representing the parameters of the model using a binary code.\n",
      "\n",
      "Another approach is to use a technique called “integer quantization,” which involves representing the parameters of the model using integers. This can\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is quantization on large language models?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f2c2c3-4e62-4f40-a5c0-d299f5dbf037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: what is the meaning of life?\n",
      "\n",
      "I’m not sure if I’ve ever asked myself this question. I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been too busy living my life to think about it.\n",
      "\n",
      "I’ve always been\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the meaning of life?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53760bbb-c269-4549-968a-17646043982c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad774447-85eb-4c9c-88d5-929a3536d04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1488a6-9b34-4b5b-83bd-ce9566d0d7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe39414-af25-4c8e-95fd-d3652f6c6d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a484f6-6ba4-4c4c-947c-3478cacdd96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f9552-8539-43a1-9648-d66ad60bedf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d385c-25a8-4492-8d32-17e056685502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
